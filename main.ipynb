{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdkpinfFSXma",
        "outputId": "6cac26f6-73cd-4795-e32c-afc45960a953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Dataset URL: https://www.kaggle.com/datasets/moeinkpr/snappfood-comments\n",
            "License(s): MIT\n",
            "Downloading snappfood-comments.zip to /content\n",
            "  0% 0.00/98.1M [00:00<?, ?B/s]\n",
            "100% 98.1M/98.1M [00:00<00:00, 1.72GB/s]\n",
            "Archive:  snappfood-comments.zip\n",
            "  inflating: comments.csv            \n",
            "  inflating: vendors.csv             \n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d moeinkpr/snappfood-comments\n",
        "\n",
        "!unzip snappfood-comments.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuDgG8UcSd3P",
        "outputId": "199b5efe-771a-42ec-82f9-04791a5fb85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 603, done.\u001b[K\n",
            "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 603 (delta 131), reused 82 (delta 82), pack-reused 434 (from 3)\u001b[K\n",
            "Receiving objects: 100% (603/603), 199.38 KiB | 9.49 MiB/s, done.\n",
            "Resolving deltas: 100% (305/305), done.\n",
            "Installing RAPIDS remaining 25.08 libraries\n",
            "Using Python 3.12.11 environment at: /usr\n",
            "Resolved 180 packages in 2.36s\n",
            "Prepared 41 packages in 53.27s\n",
            "Uninstalled 31 packages in 1.38s\n",
            "Installed 41 packages in 826ms\n",
            " - bokeh==3.7.3\n",
            " + bokeh==3.6.3\n",
            " + cucim-cu12==25.8.0\n",
            " + cuda-bindings==12.9.3\n",
            " + cuda-pathfinder==1.3.0\n",
            " - cuda-python==12.6.2.post1\n",
            " + cuda-python==12.9.3\n",
            " - cudf-cu12==25.6.0 (from https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.6.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl)\n",
            " + cudf-cu12==25.8.0\n",
            " + cugraph-cu12==25.8.0\n",
            " - cuml-cu12==25.6.0\n",
            " + cuml-cu12==25.8.0\n",
            " - cuvs-cu12==25.6.1\n",
            " + cuvs-cu12==25.8.0\n",
            " + cuxfilter-cu12==25.8.0\n",
            " - dask==2025.5.0\n",
            " + dask==2025.7.0\n",
            " - dask-cuda==25.6.0\n",
            " + dask-cuda==25.8.0\n",
            " - dask-cudf-cu12==25.6.0\n",
            " + dask-cudf-cu12==25.8.0\n",
            " + datashader==0.18.2\n",
            " - distributed==2025.5.0\n",
            " + distributed==2025.7.0\n",
            " - distributed-ucxx-cu12==0.44.0\n",
            " + distributed-ucxx-cu12==0.45.1\n",
            " - holoviews==1.21.0\n",
            " + holoviews==1.20.2\n",
            " + jupyter-server-proxy==4.4.0\n",
            " - libcudf-cu12==25.6.0 (from https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.6.0-py3-none-manylinux_2_28_x86_64.whl)\n",
            " + libcudf-cu12==25.8.0\n",
            " - libcugraph-cu12==25.6.0\n",
            " + libcugraph-cu12==25.8.0\n",
            " - libcuml-cu12==25.6.0\n",
            " + libcuml-cu12==25.8.0\n",
            " - libcuvs-cu12==25.6.1\n",
            " + libcuvs-cu12==25.8.0\n",
            " - libkvikio-cu12==25.6.0\n",
            " + libkvikio-cu12==25.8.0\n",
            " - libraft-cu12==25.6.0\n",
            " + libraft-cu12==25.8.0\n",
            " - librmm-cu12==25.6.0\n",
            " + librmm-cu12==25.8.0\n",
            " - libucxx-cu12==0.44.0\n",
            " + libucxx-cu12==0.45.1\n",
            " - numba-cuda==0.11.0\n",
            " + numba-cuda==0.14.1\n",
            " + nvidia-cuda-cccl-cu12==12.9.27\n",
            " - nx-cugraph-cu12==25.6.0 (from https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.6.0-py3-none-any.whl)\n",
            " + nx-cugraph-cu12==25.8.0\n",
            " - panel==1.8.2\n",
            " + panel==1.7.5\n",
            " + pyct==0.6.0\n",
            " - pylibcudf-cu12==25.6.0 (from https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.6.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl)\n",
            " + pylibcudf-cu12==25.8.0\n",
            " - pylibcugraph-cu12==25.6.0\n",
            " + pylibcugraph-cu12==25.8.0\n",
            " - pylibraft-cu12==25.6.0\n",
            " + pylibraft-cu12==25.8.0\n",
            " - raft-dask-cu12==25.6.0\n",
            " + raft-dask-cu12==25.8.0\n",
            " - rapids-dask-dependency==25.6.0\n",
            " + rapids-dask-dependency==25.8.0\n",
            " - rmm-cu12==25.6.0\n",
            " + rmm-cu12==25.8.0\n",
            " - shapely==2.1.2\n",
            " + shapely==2.0.7\n",
            " + simpervisor==1.0.0\n",
            " - ucx-py-cu12==0.44.0\n",
            " + ucx-py-cu12==0.45.0\n",
            " - ucxx-cu12==0.44.0\n",
            " + ucxx-cu12==0.45.1\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "\n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "\n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install hazm library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pzFCbRycScu7",
        "outputId": "32c949dd-45e9-4979-b212-0d991e1b2228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: hazm 0.10.0\n",
            "Uninstalling hazm-0.10.0:\n",
            "  Successfully uninstalled hazm-0.10.0\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Collecting hazm\n",
            "  Using cached hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Using cached hazm-0.10.0-py3-none-any.whl (892 kB)\n",
            "Installing collected packages: hazm\n",
            "Successfully installed hazm-0.10.0\n",
            "Collecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting fasttext-wheel\n",
            "  Downloading fasttext_wheel-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel)\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from fasttext-wheel) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fasttext-wheel) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasttext_wheel-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9300 sha256=8b4a10acc8250f074e137f80ea28ac244fdf3018a813b3568f2b977a2df4e866\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/24/da/4d994d7a27cfc73a4e513a669fbeec4a71f871fe245a81977f\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext, scipy, python-crfsuite, pybind11, gensim, fasttext-wheel\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "hazm 0.10.0 requires numpy==1.24.3, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 gensim-4.3.3 pybind11-3.0.1 python-crfsuite-0.9.11 scipy-1.13.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ee3991da303544aa82cc5b074645a81a",
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#در این قسمت ابتدا سه لایبرری زیر را نصب می کنیم. بعد از اتمام نصب خط زیر را از کامنت در آورده و مجدد این سلول را ران کنید\n",
        "#اگر چه بهتر است که کتابخانه hazm بر روی پایتون 3.10 نصب شود، اما می توان آن را بر روی 3.12 هم داشت.\n",
        "# !pip uninstall -y hazm\n",
        "!pip install numpy==1.26.4\n",
        "!pip install hazm --no-deps\n",
        "!pip install python-crfsuite fasttext-wheel gensim nltk flashtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6cmaDl88m0l"
      },
      "outputs": [],
      "source": [
        "import cudf\n",
        "from cuml.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import cudf\n",
        "from hazm import Normalizer, Lemmatizer, word_tokenize\n",
        "from cuml.cluster import KMeans\n",
        "from gensim.models import Word2Vec\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaajhMbuRTBz"
      },
      "outputs": [],
      "source": [
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "comments_table = cudf.read_csv(\"comments.csv\")\n",
        "vendors_table = cudf.read_csv(\"vendors.csv\")\n",
        "# فیلد هایی که ارزش تحلیلی ندارند را حذف می کنیم تا جچم دیتا کاهش یابد و مدل سریع تر و بهینه تر آموزش یابد\n",
        "\n",
        "comments_table = comments_table.drop(columns=[\"commentId\",\"date\",\"sender\",\"rating\",\"customerId\",\"feeling\",\"status\",\"foods\",\"replies\"], errors=\"ignore\")\n",
        "vendors_table = vendors_table.drop(columns=[\"id\",\"highlight\",\"description\",\"address\",\"rating\",\"title\"], errors=\"ignore\")\n",
        "# دو جدول را بر اساس کد رستوران مرج می کنیم\n",
        "\n",
        "merged_df = comments_table.merge(vendors_table, on=\"code\", how=\"left\")\n",
        "# در دیتاست فقط رستوران ها رو انتخاب می کنیم و بعد از آن ستون vendorType را حذف می کنیم\n",
        "\n",
        "merged_df = merged_df[merged_df[\"vendorType\"] == \"RESTAURANT\"]\n",
        "merged_df = merged_df.drop(columns=[\"vendorType\"])\n",
        "\n",
        "# دو ستون که مربط به کامنت ها بودند را مرج می کنیم تا همه کامنت ها را بررسی کنیم\n",
        "# در ستون deliveryComment مقدار زیادی فیلد خالی وجود دارد\n",
        "# برای این که در توسعه مدل اختلالی  وجود نیاید آن را با یک رشته خالی پر می کنیم\n",
        "\n",
        "merged_df[\"deliveryComment\"] = merged_df[\"deliveryComment\"].fillna(\"\")\n",
        "merged_df[\"commentText\"] = merged_df[\"commentText\"].fillna(\"\")\n",
        "\n",
        "merged_df[\"text_raw\"] = merged_df[\"commentText\"] + \" \" + merged_df[\"deliveryComment\"]\n",
        "merged_df = merged_df.drop(columns=[\"commentText\", \"deliveryComment\"])\n",
        "\n",
        "# چون هدف ما این است که بصورت هفتگی روند رستوران ها را بررسی کنیم ستونی که مربوط به زمان ایجاد کامنت هست را به سال و هفته تقسیم می کنیم\n",
        "\n",
        "merged_df[\"createdDate\"] = cudf.to_datetime(merged_df[\"createdDate\"])\n",
        "merged_df[\"week\"] = merged_df[\"createdDate\"].dt.isocalendar().week.astype(\"int32\")\n",
        "merged_df[\"year\"] = merged_df[\"createdDate\"].dt.year.astype(\"int32\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# سفارش ها به چندین صورت به کاربر می رسد. به همین دلیل آن ها را انکود می کنیم تا حجم دیتا کاهش یابد\n",
        "\n",
        "le = LabelEncoder()\n",
        "merged_df[\"expeditionType_encoded\"] = le.fit_transform(merged_df[\"expeditionType\"].astype(\"str\"))\n",
        "merged_df = merged_df.drop(columns=[\"expeditionType\"])\n",
        "\n",
        "merged_df = merged_df.dropna(subset=[\"rate\"])\n",
        "\n",
        "merged_df[\"rate\"] = merged_df[\"rate\"].astype(\"uint8\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data type casting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiJJeutzTIYJ",
        "outputId": "a133f21a-1ac2-466a-9ea6-8acbc426ec92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'cudf.core.dataframe.DataFrame'>\n",
            "Index: 1078372 entries, 16 to 1290548\n",
            "Data columns (total 8 columns):\n",
            " #   Column                  Non-Null Count    Dtype\n",
            "---  ------                  --------------    -----\n",
            " 0   createdDate             1078372 non-null  datetime64[ns]\n",
            " 1   rate                    1078372 non-null  uint8\n",
            " 2   code                    1078372 non-null  object\n",
            " 3   commentCount            1078372 non-null  object\n",
            " 4   text_raw                1078372 non-null  object\n",
            " 5   week                    1078372 non-null  uint8\n",
            " 6   year                    1078372 non-null  uint16\n",
            " 7   expeditionType_encoded  1078084 non-null  uint8\n",
            "dtypes: datetime64[ns](1), object(3), uint16(1), uint8(3)\n",
            "memory usage: 151.6+ MB\n"
          ]
        }
      ],
      "source": [
        "#با استفاده از این تابع همه داده ها را بررسی کرده و تا جایی که امکان دارد آن ها را کاهش حجم می دهیم\n",
        "\n",
        "def auto_data_type(df):\n",
        "    for col in df.select_dtypes(include=[np.number]).columns:\n",
        "        min_value = df[col].min()\n",
        "        max_value = df[col].max()\n",
        "\n",
        "        if pd.api.types.is_float_dtype(df[col]):\n",
        "            df[col] = df[col].astype(np.float32)\n",
        "        elif min_value >= 0:\n",
        "            if max_value <= 255:\n",
        "                df[col] = df[col].astype(np.uint8)\n",
        "            elif max_value <= 65535:\n",
        "                df[col] = df[col].astype(np.uint16)\n",
        "            elif max_value <= 4294967295:\n",
        "                df[col] = df[col].astype(np.uint32)\n",
        "            else:\n",
        "                df[col] = df[col].astype(np.uint64)\n",
        "        else:\n",
        "            if -128 <= min_value and max_value <= 127:\n",
        "                df[col] = df[col].astype(np.int8)\n",
        "            elif -32768 <= min_value and max_value <= 32767:\n",
        "                df[col] = df[col].astype(np.int16)\n",
        "            elif -2147483648 <= min_value and max_value <= 2147483647:\n",
        "                df[col] = df[col].astype(np.int32)\n",
        "            else:\n",
        "                df[col] = df[col].astype(np.int64)\n",
        "    return df\n",
        "\n",
        "\n",
        "merged_df=auto_data_type(merged_df)\n",
        "\n",
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download comprehensive stop_words list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFm0g3zwTWrS",
        "outputId": "1ded6c88-9574-40ea-8474-97b2e6ef5e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/alioraji/persian-stop-words\n",
            "License(s): other\n",
            "persian-stop-words.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  persian-stop-words.zip\n",
            "replace Persian_Stop_Words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d alioraji/persian-stop-words\n",
        "!unzip persian-stop-words.zip\n",
        "with open(\"Persian_Stop_Words.txt\", encoding='utf-8') as f:\n",
        "    custom_stopwords = {line.strip() for line in f if line.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Process on text and Categorize words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TThxEOPaRaUe",
        "outputId": "62ca520f-2c5a-4eb8-e8c5-804a4c1f4f55"
      },
      "outputs": [],
      "source": [
        "#در این تابع استاندارد سازی و نرمال سازی متن را انجام می دهیم\n",
        "def preprocess_text(text, stopwords):\n",
        "    #متن نرمال سازی می شود\n",
        "    text = normalizer.normalize(str(text))\n",
        "    # کاراکتر های غیر فارسی حذف می شوند مانند علائم نگارشی یا اموجی ها\n",
        "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \" \", text)\n",
        "    # اعداد  از متن حذف می شود\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    # فاصله های اضافی حذف می شود\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # متن را به یک سری توکن تبدیل می کنیم\n",
        "    tokens = word_tokenize(text)\n",
        "    # کلمات پر تکرار و بی معنی حذف می شود\n",
        "    tokens = [t for t in tokens if t not in stopwords and len(t) > 2]\n",
        "    # کلمات ریشه یابی می شود\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# تابع را بر روی همه کامنت ها اعمال می شود. به دلیل آن که کتابخانه hazm بر روی cpu احرا می شود، این فرآیند یک مقدار زمان بر است.\n",
        "text_series = merged_df[\"text_raw\"].to_pandas()\n",
        "text_clean = text_series.progress_apply(lambda x: preprocess_text(x, custom_stopwords))\n",
        "merged_df[\"text_clean\"] = cudf.Series(text_clean)\n",
        "\n",
        "# هر جمله را به یک سری کلمات شکسته می شود تا آن ها را با استفاده از word2vec به بردار عددی تیدیل کنیم\n",
        "sentences = [t.split() for t in text_clean if isinstance(t, str) and len(t) > 0]\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=1, workers=4, seed=42)\n",
        "\n",
        "# پس از محاسبه بردار های عددی، 1500 کلمه اول را که پر تکرار بودند را انتخاب می کنیم\n",
        "all_words = [w for s in sentences for w in s]\n",
        "word_freq = (pd.Series(all_words).value_counts().reset_index().rename(columns={\"index\": \"word\", \"word\": \"count\"}))\n",
        "common_words = [ w for w in word_freq[\"word\"].head(1500) if w in w2v_model.wv]\n",
        "\n",
        "#در این بخش، تعداد خوشه‌ها  را 20 تا در نظر گرفتیم و دسته ‌بندی‌های اصلی را 6 تا. دلیل این کار این است که الگوریتم KMeans به‌صورت خودکار ، فقط بر اساس شباهت عددی بین بردارهای کلمات، آن‌ها را در چند خوشه قرار می‌دهد تا واژه‌های مشابه از نظر معنا کنار هم بیایند. اما ۶ دسته‌ی اصلی مانند کیفیت، قیمت، ارسال و ...، دسته‌ های از پیش‌ تعریف ‌شده‌ای هستند که در نظر گرفته‌ شد (می توان دسته های بیشتری نیز اضافه کرد). بصورت کلی ، ابتدا مدل ۲۰ خوشه‌ می‌سازد تا کلمات نزدیک به هم را پیدا کند، سپس این خوشه‌ ها را در قالب ۶ دسته بندی طبقه‌ بندی می شود.\n",
        "\n",
        "word_vectors = cudf.DataFrame([w2v_model.wv[w] for w in common_words])\n",
        "\n",
        "kmeans = KMeans(n_clusters=20, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(word_vectors)\n",
        "\n",
        "clusters = {}\n",
        "for w, lbl in zip(common_words, labels.to_pandas()):\n",
        "    lbl = int(lbl)\n",
        "    if lbl not in clusters:\n",
        "        clusters[lbl] = []\n",
        "    clusters[lbl].append(w)\n",
        "\n",
        "categories = {\n",
        "    \"quality\": [\"کیفیت\", \"خوشمزه\", \"عالی\", \"بد\", \"راضی\", \"طعم\"],\n",
        "    \"delivery\": [\"تحویل\", \"پیک\", \"ارسال\", \"دیر\", \"زود\", \"سریع\"],\n",
        "    \"price\": [\"قیمت\", \"گرون\", \"ارزون\", \"هزینه\", \"پول\"],\n",
        "    \"packaging\": [\"بسته\", \"بسته‌بندی\", \"جعبه\", \"ظرف\", \"کیسه\"],\n",
        "    \"amount\": [\"حجم\", \"مقدار\", \"کم\", \"زیاد\", \"پرس\"],\n",
        "    \"order_error\": [\"اشتباه\", \"جا\", \"فراموش\", \"غلط\", \"نداده\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMG8D1QvX7hf"
      },
      "outputs": [],
      "source": [
        "for topic, words in categories.items():\n",
        "    pattern = '|'.join(words)\n",
        "    merged_df[f'flag_{topic}'] = merged_df['text_clean'].str.contains(pattern, regex=True).astype(\"uint8\")\n",
        "\n",
        "flags = list(categories.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train and test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd0plv22SERV"
      },
      "outputs": [],
      "source": [
        "# اگر امتیاز کامنت بیشتر از 4 باشد مثبت در نظر گرفته می شود و در غیر ای ن صورت منفی در نظر گرفته می شود. دلیل این کار این است که مدل بهتر بتواند متوجه شود چه کامنتی مثبت یا منفی بوده است و عوامل تاثیر گذار ذر آن کامنت  را درک کند.\n",
        "merged_df[\"rating_class\"] = (merged_df[\"rate\"] >= 4).astype(\"int32\")\n",
        "flags = [\"quality\", \"delivery\", \"price\", \"packaging\", \"amount\", \"order_error\"]\n",
        "\n",
        "results = []\n",
        "\n",
        "# در این بخش صرفا رستوران هایی که تعداد کامنت های شان بیشتر از 30 عدد هست را انتخاب می کنیم\n",
        "for code, group in merged_df.groupby(\"code\"):\n",
        "    if len(group) < 30:\n",
        "        continue\n",
        "\n",
        "# .اگر در رستورانی همه کامنت ها مثبت یا منفی بود نوع تقسیم بندی داده ها در ترین و تست مدل را عوض می کنیم.\n",
        "# به عبارت دیگر چک می کنیم آیا کامنت مثبت و منفی هر دو برای آن رستوران وحود دارد یا خیر\n",
        "    counts = group[\"rating_class\"].value_counts()\n",
        "    if len(counts) < 2 or counts.min() < 2:\n",
        "        stratify_flag = False\n",
        "    else:\n",
        "        stratify_flag = True\n",
        "\n",
        "    X = group[[f\"flag_{f}\" for f in flags]].astype(\"float32\").to_pandas()\n",
        "    y = group[\"rating_class\"].to_pandas()\n",
        "\n",
        "    if stratify_flag:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y )\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 )\n",
        "\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        continue\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        n_estimators=1000,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        device=\"cuda\",\n",
        "        random_state=42,\n",
        "        scale_pos_weight=len(y_train) / max(y_train.sum(), 1),\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "# سه عامل تاثیر گذار در امتیاز را انتخاب می کنیم\n",
        "    importances = model.feature_importances_\n",
        "    top3 = [f for f, v in sorted(zip(flags, importances), key=lambda x: -x[1])[:3]]\n",
        "\n",
        "    results.append({ \"code\": code,\"top_factors\": \", \".join(top3),\"accuracy\": acc,\"precision\": prec,\"recall\": rec,\"f1\": f1 })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpdBFB_kat9B",
        "outputId": "495297e3-a860-4277-f627-00032d4e1508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒════════════╤═════════════╤══════════╤════════╕\n",
            "│   Accuracy │   Precision │   Recall │     F1 │\n",
            "╞════════════╪═════════════╪══════════╪════════╡\n",
            "│     0.8299 │      0.8321 │   0.9897 │ 0.9004 │\n",
            "╘════════════╧═════════════╧══════════╧════════╛\n"
          ]
        }
      ],
      "source": [
        "mean_accuracy = np.mean([r[\"accuracy\"] for r in results])\n",
        "mean_precision = np.mean([r[\"precision\"] for r in results])\n",
        "mean_recall = np.mean([r[\"recall\"] for r in results])\n",
        "mean_f1 = np.mean([r[\"f1\"] for r in results])\n",
        "\n",
        "mean_table = [[\n",
        "    round(mean_accuracy, 4),\n",
        "    round(mean_precision, 4),\n",
        "    round(mean_recall, 4),\n",
        "    round(mean_f1, 4),\n",
        "]]\n",
        "\n",
        "print(tabulate(mean_table, headers=[\"Accuracy\", \"Precision\", \"Recall\", \"F1\"], tablefmt=\"fancy_grid\"))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
